Sun Jun  4 16:12:21 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A30          On   | 00000000:03:00.0 Off |                    0 |
| N/A   20C    P0    27W / 165W |      0MiB / 24576MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A30          On   | 00000000:85:00.0 Off |                    0 |
| N/A   37C    P0   100W / 165W |    285MiB / 24576MiB |     53%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    1   N/A  N/A    219590      C                                     283MiB |
+-----------------------------------------------------------------------------+
Activando entorno virtual
Entorno virtual activado
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2023/06/04 16:12:50 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.
The git executable must be specified in one of the following ways:
    - be included in your $PATH
    - be set via $GIT_PYTHON_GIT_EXECUTABLE
    - explicitly set via git.refresh()

All git commands will error until this is rectified.

This initial warning can be silenced or aggravated in the future by setting the
$GIT_PYTHON_REFRESH environment variable. Use one of the following values:
    - quiet|q|silence|s|none|n|0: for no warning or exception
    - warn|w|warning|1: for a printed warning
    - error|e|raise|r|2: for a raised exception

Example:
    export GIT_PYTHON_REFRESH=quiet

You are using a CUDA device ('NVIDIA A30') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name    | Type       | Params
---------------------------------------
0 | network | Sequential | 21.6 M
---------------------------------------
21.6 M    Trainable params
0         Non-trainable params
21.6 M    Total params
86.517    Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
2023/06/04 16:29:08 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu117) contains a local version label (+cu117). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.
2023/06/04 16:29:36 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpdd0z91lu/model/data, flavor: pytorch), fall back to return ['torch==2.0.1', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.
/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:148: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.
  rank_zero_warn(
You are using a CUDA device ('NVIDIA A30') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Restoring states from the checkpoint path at /home/mnsosa/CHB-MIT_Seizure_Prediction/585718752977019583/885c84ae0c864736a2574ecc3adc1d00/checkpoints/epoch=5-step=210.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Traceback (most recent call last):
  File "/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/mlflow/store/tracking/file_store.py", line 990, in log_batch
    self._log_run_param(run_info, param)
  File "/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/mlflow/store/tracking/file_store.py", line 895, in _log_run_param
    self._validate_new_param_value(
  File "/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/mlflow/store/tracking/file_store.py", line 915, in _validate_new_param_value
    raise MlflowException(
mlflow.exceptions.MlflowException: Changing param values is not allowed. Param with key='train_data' was already logged with value='<torch.utils.data.dataset.Subset object at 0x14d65d24eed0>' for run ID='885c84ae0c864736a2574ecc3adc1d00'. Attempted logging new value '<torch.utils.data.dataset.Subset object at 0x14d664dd5510>'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/mnsosa/CHB-MIT_Seizure_Prediction/src/train.py", line 66, in <module>
    trainer.test()
  File "/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 706, in test
    return call._call_and_handle_interrupt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 749, in _test_impl
    results = self._run(model, ckpt_path=ckpt_path)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 918, in _run
    _log_hyperparams(self)
  File "/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/pytorch_lightning/loggers/utilities.py", line 94, in _log_hyperparams
    logger.log_hyperparams(hparams_initial)
  File "/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/lightning_utilities/core/rank_zero.py", line 27, in wrapped_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/pytorch_lightning/loggers/mlflow.py", line 251, in log_hyperparams
    self.experiment.log_batch(run_id=self.run_id, params=params_list[idx : idx + 100])
  File "/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/mlflow/tracking/client.py", line 965, in log_batch
    self._tracking_client.log_batch(run_id, metrics, params, tags)
  File "/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/client.py", line 366, in log_batch
    self.store.log_batch(
  File "/home/mnsosa/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/mlflow/store/tracking/file_store.py", line 1001, in log_batch
    raise MlflowException(e, INTERNAL_ERROR)
mlflow.exceptions.MlflowException: Changing param values is not allowed. Param with key='train_data' was already logged with value='<torch.utils.data.dataset.Subset object at 0x14d65d24eed0>' for run ID='885c84ae0c864736a2574ecc3adc1d00'. Attempted logging new value '<torch.utils.data.dataset.Subset object at 0x14d664dd5510>'.
